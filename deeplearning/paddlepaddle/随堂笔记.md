# 随堂笔记

## 1-1

### 三大应用

* speech/audio processing
* computer vision
* natural language processing
  
### 崛起三要素

* 算法
* 大数据
* 计算力

### 关系

人工智能 > 机器学习 > 深度学习

### 学习方式

| 监督学习 | 无监督学习 | 增强学习 |
| -------- | ---------- | -------- |
| 人脸识别 | 文本自聚类 | 游戏     |
| 有监督   | 无监督     |

## 1-2

环境搭建，没了

## 1-3

yield 基础题，没了

## 2-1

深度学习 == 人工神经网络，最小单元：感知机（神经元）

y hat 预测值 y 真实值（标签）

神经网络乘数 == 含权重层层数

fc 全连接层 w, b 参数

### g 激活函数

* sigmoid 易出现梯度消失 0～1

* tanh 易出现梯度消失 -1～1

* ReLU 隐藏层常用 0 > 信息丢失

* linear 仅用于线性回归

* softmax 只用于多分类问题最后一层

### 损失函数

* 均方误差代价函数

* 交叉熵损失函数

w, b 非 0 且足够小，b 有时可为 0

全部 w, b 不能一样

a 为学习率（超参数）

### 梯度下降

* 批量梯度下降 BGD

  局部最优 样本多训练慢

* 随机梯度下降法 SGD

  训练速度快 盲目 准确度低 迭代次数增加

* 小批量梯度下降 MBGD

  更新使用 b 个样本 b 为超参数

```python
CUDAPlace(0) # 第 0 块卡

act = # 激活函数

fluid.default_startup_program() # 初始化 w, b

BATCH_SIZE = # 一次训练数量
```

一轮循环 == 用完所有数据

## 2-2

双 for 写法

## 2-3

练习

## 3-1

trainer 写法

## 3-2

`np.argsort` 返回数组值从小到大的索引值

### `BN` 数据批归一化

缓解梯度消失 对每层输入归一化

dropout 应对过拟合

pool 池化

padding 补 0

conv 卷积层

* valid 卷积核完全在信号内

* same 卷积核中心在信号内

* full 卷积核边沿在信号内

stride 平移步长

filter 二维卷积核

feature map 特征图谱

## 3-3

写题

## 4-1

在信号处理领域，要先对卷积核做逆序处理，来满足交换定律

深度学习则不用

VGG 通常有 VGG 16，VGG 19

3 个 `3*3` 卷积核比 1 个 `7*7` 卷积核优势在于：

1. 包括三个 ReLu 层而不是一个，使决策函数更有判别性

2. 减少了参数，如输入输出都是 C 个通道，

   使用 `3*3` 需要 `3*(3*3*C*C) = 27*C*C` 个参数

   使用 `7*7` 需要 `7*7*C*C = 49*C*C` 个参数

网络深问题： 梯度弥散，梯度爆炸，更高训练误差

shortcut 捷径

欠拟合

过拟合：训练误差很小，但是测试误差很大

泛化：模型能对预测数据有较好效果

数据填充：

* 水平翻转

* 随机抠图 80～90%

* 旋转 15度 30度 45度

* 色彩抖动

数据预处理

为什么要迁移学习？

1. 缺少大量数据

2. 大量数据缺少标注

3. 硬件设施算力较弱

4. 特定领域应用的需求

| 比较项目 | 传统机器学习                 | 迁移学习                     |
| -------- | ---------------------------- | ---------------------------- |
| 数据分布 | 训练和测试数据服从相同的分布 | 训练和测试数据服从不同的分布 |
| 数据标注 | 需要足够的数据标注来训练模型 | 不需要足够的数据标注         |
| 模型     | 每个任务分别建模             | 模型可以在不同任务之间迁移   |

`Domain` 域

`source domain` 源域

`target domain` 目标域

迁移学习总体思路：开发算法来最大限度地利用有标注的领域的知识，来辅助目标领域的知识获取和学习

找到相似性 => 找到不变量

`finetune` 微调，利用别人已经训练好的网络，针对自己的任务再进行调整
